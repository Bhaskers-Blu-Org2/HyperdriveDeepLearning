{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adopted from https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Script\n",
    "\n",
    "In this notebook, we create the training script by preparing the hyperparameters of the MaskRCNN model that will be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import sys \n",
    "import os\n",
    "import argparse\n",
    "sys.path.append('./cocoapi/PythonAPI/')\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "\n",
    "import transforms as T\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset class and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append train.py\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, 'JPEGImages'))))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, 'JPEGImages', self.imgs[idx])\n",
    "        xml_path = os.path.join(self.root, 'Annotations', '{}.xml'.format(self.imgs[idx].strip('.jpg')))\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # parse XML annotation\n",
    "        tree = ET.parse(xml_path)\n",
    "        t_root = tree.getroot()\n",
    "        \n",
    "        # get bounding box coordinates\n",
    "        boxes = []\n",
    "        for obj in t_root.findall('object'):\n",
    "            bnd_box = obj.find('bndbox')\n",
    "            xmin = float(bnd_box.find('xmin').text)\n",
    "            xmax = float(bnd_box.find('xmax').text)\n",
    "            ymin = float(bnd_box.find('ymin').text)\n",
    "            ymax = float(bnd_box.find('ymax').text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        num_objs = len(boxes)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
    "        \n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        # area of the bounding box, used during evaluation with the COCO metric for small, medium and large boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "      \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important parameters to tune for object detection models in general are ....\n",
    "\n",
    "See following for all [arguments of MaskRCNN](https://github.com/pytorch/vision/blob/7716aba57e6e12a544c42136b274508955526163/torchvision/models/detection/mask_rcnn.py#L20) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append train.py\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Object Detection Training')\n",
    "    parser.add_argument('--data_path', \n",
    "                        default='./BuildData/', help='the path to the dataset')\n",
    "    parser.add_argument('--batch_size', \n",
    "                        default=2, type=int)\n",
    "    parser.add_argument('--epochs', \n",
    "                        default=10, type=int, help='number of total epochs to run')\n",
    "    parser.add_argument('--workers', \n",
    "                        default=4, type=int, help='number of data loading workers')\n",
    "    parser.add_argument('--learning_rate', \n",
    "                        default=0.005, type=float, help='initial learning rate')\n",
    "    parser.add_argument('--momentum', \n",
    "                        default=0.9, type=float, help='momentum')\n",
    "    parser.add_argument('--weight_decay', \n",
    "                        default=0.0005, type=float, help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('--lr_step_size', \n",
    "                        default=3, type=int, help='decrease lr every step-size epochs')\n",
    "    parser.add_argument('--lr_gamma', \n",
    "                        default=0.1, type=float, help='decrease lr by a factor of lr-gamma')\n",
    "    parser.add_argument('--print_freq', \n",
    "                        default=10, type=int, help='print frequency')\n",
    "    parser.add_argument('--output_dir', \n",
    "                        default='outputs', help='path where to save')\n",
    "    parser.add_argument('--anchor_sizes', \n",
    "                        default='16', type=str, help='anchor sizes')\n",
    "    parser.add_argument('--anchor_aspect_ratios', \n",
    "                        default= '1.0', type=str, help='anchor aspect ratios')\n",
    "    parser.add_argument('--rpn_nms_thresh', \n",
    "                        default= 0.7, type=float,  help='NMS threshold used for postprocessing the RPN proposals')\n",
    "    parser.add_argument('--box_nms_thresh', \n",
    "                        default= 0.5, type=float,  help='NMS threshold for the prediction head. Used during inference')\n",
    "    parser.add_argument('--box_score_thresh', \n",
    "                        default= 0.05, type=float,  help='during inference only return proposals' \n",
    "                        'with a classification score greater than box_score_thresh')\n",
    "    parser.add_argument('--box_detections_per_img', \n",
    "                        default= 100, type=int,  help='maximum number of detections per image, for all classes')\n",
    "    args = parser.parse_args() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append train.py\n",
    "\n",
    "data_path = args.data_path\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = BuildDataset(data_path, get_transform(train=True))\n",
    "dataset_test = BuildDataset(data_path, get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-100])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-100:])\n",
    "\n",
    "batch_size = args.batch_size\n",
    "workers = args.workers\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=2, shuffle=False, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append train.py\n",
    "\n",
    "# our dataset has two classes only - background and out of stock\n",
    "num_classes = 2\n",
    "\n",
    "rpn_nms_threshold = args.rpn_nms_thresh\n",
    "box_nms_threshold = args.box_nms_thresh\n",
    "box_score_threshold = args.box_score_thresh\n",
    "num_box_detections = args.box_detections_per_img\n",
    "\n",
    "# load pre-trained maskRCNN model\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, \n",
    "                                                           rpn_nms_thresh=rpn_nms_threshold,\n",
    "                                                           box_nms_thresh=box_nms_threshold, \n",
    "                                                           box_score_thresh=box_score_threshold,\n",
    "                                                           box_detections_per_img=num_box_detections)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "anchor_sizes = args.anchor_sizes\n",
    "anchor_sizes = tuple([float(i) for i in anchor_sizes.split(',')])\n",
    "anchor_aspect_ratios = args.anchor_aspect_ratios\n",
    "anchor_aspect_ratios = tuple([float(i) for i in anchor_aspect_ratios.split(',')])\n",
    "\n",
    "# create an anchor_generator for the FPN which by default has 5 outputs\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=tuple([anchor_sizes for _ in range(5)]),\n",
    "    aspect_ratios=tuple([anchor_aspect_ratios for _ in range(5)]))\n",
    "model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "# get number of input features for the RPN returned by FPN (256)\n",
    "in_channels = model.backbone.out_channels\n",
    "\n",
    "# replace the RPN head \n",
    "model.rpn.head = RPNHead(in_channels, anchor_generator.num_anchors_per_location()[0])\n",
    "\n",
    "# turn off masks since dataset only has bounding boxes\n",
    "model.roi_heads.mask_roi_pool = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append train.py\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = args.learning_rate\n",
    "momentum = args.momentum\n",
    "weight_decay = args.weight_decay\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=learning_rate,\n",
    "                            momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "lr_step_size = args.lr_step_size\n",
    "lr_gamma = args.lr_gamma\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=lr_step_size,\n",
    "                                               gamma=lr_gamma)\n",
    "\n",
    "# number of training epochs\n",
    "num_epochs = args.epochs\n",
    "print_freq = args.print_freq\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=print_freq)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset after every epoch\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "    \n",
    "#save model\n",
    "torch.save(model.state_dict(), os.path.join(args.output_dir, 'model_latest.pth'))\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python train.py --data_path /datadrive/torchvisionOD/BuildData/ --workers 8 --epochs 5 --anchor_sizes 16,32,64,128,256,512 --anchor_aspect_ratios 0.25,0.5,1.0,2.0 --rpn_nms_thresh 0.5 --box_nms_thresh 0.3 --box_score_thresh 0.10\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchdetectaml]",
   "language": "python",
   "name": "conda-env-torchdetectaml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
