{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf74d2e9-2708-49b1-934b-e0ede342f475"
    }
   },
   "source": [
    "# How to use Estimator in Azure ML\n",
    "\n",
    "## Introduction\n",
    "This tutorial shows how to use the Estimator pattern in Azure Machine Learning SDK. Estimator is a convenient object in Azure Machine Learning that wraps run configuration information to help simplify the tasks of specifying how a script is executed.\n",
    "\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First let's import some Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "edaa7f2f-2439-4148-b57a-8c794c0945ec"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.41\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "# print('Workspace name: ' + ws.name, \n",
    "#       'Azure region: ' + ws.location, \n",
    "#       'Subscription id: ' + ws.subscription_id, \n",
    "#       'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "59f52294-4a25-4c92-bab8-3b07f0f44d15"
    }
   },
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment named \"estimator-test\". The script runs will be recorded under this experiment in Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "bc70f780-c240-4779-96f3-bc5ef9a37d59"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(workspace=ws, name='torchvision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2039d2d5-aca6-4f25-a12f-df9ae6529cae"
    }
   },
   "source": [
    "## Use a train.py script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import sys \n",
      "import os\n",
      "import argparse\n",
      "sys.path.append('./cocoapi/PythonAPI/')\n",
      "\n",
      "from PIL import Image\n",
      "import torch\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "import torchvision\n",
      "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
      "from torchvision.models.detection.rpn import AnchorGenerator\n",
      "from torchvision.models.detection.rpn import RPNHead\n",
      "\n",
      "import transforms as T\n",
      "from engine import train_one_epoch, evaluate\n",
      "import utils\n",
      "\n",
      "class BuildDataset(torch.utils.data.Dataset):\n",
      "    def __init__(self, root, transforms=None):\n",
      "        self.root = root\n",
      "        self.transforms = transforms\n",
      "        # load all image files\n",
      "        self.imgs = list(sorted(os.listdir(os.path.join(root, 'JPEGImages'))))\n",
      "        \n",
      "    def __getitem__(self, idx):\n",
      "        img_path = os.path.join(self.root, 'JPEGImages', self.imgs[idx])\n",
      "        xml_path = os.path.join(self.root, 'Annotations', '{}.xml'.format(self.imgs[idx].strip('.jpg')))\n",
      "        img = Image.open(img_path).convert(\"RGB\")\n",
      "        \n",
      "        # parse XML annotation\n",
      "        tree = ET.parse(xml_path)\n",
      "        t_root = tree.getroot()\n",
      "        \n",
      "        # get bounding box coordinates\n",
      "        boxes = []\n",
      "        for obj in t_root.findall('object'):\n",
      "            bnd_box = obj.find('bndbox')\n",
      "            xmin = float(bnd_box.find('xmin').text)\n",
      "            xmax = float(bnd_box.find('xmax').text)\n",
      "            ymin = float(bnd_box.find('ymin').text)\n",
      "            ymax = float(bnd_box.find('ymax').text)\n",
      "            boxes.append([xmin, ymin, xmax, ymax])\n",
      "        num_objs = len(boxes)\n",
      "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
      "        \n",
      "        # there is only one class\n",
      "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
      "        image_id = torch.tensor([idx])\n",
      "\n",
      "        # area of the bounding box, used during evaluation with the COCO metric for small, medium and large boxes\n",
      "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
      "        \n",
      "        # suppose all instances are not crowd\n",
      "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
      "        \n",
      "        target = {}\n",
      "        target[\"boxes\"] = boxes\n",
      "        target[\"labels\"] = labels\n",
      "        target[\"image_id\"] = image_id\n",
      "        target[\"area\"] = area\n",
      "        target[\"iscrowd\"] = iscrowd\n",
      "        \n",
      "        if self.transforms is not None:\n",
      "            img, target = self.transforms(img, target)\n",
      "      \n",
      "        return img, target\n",
      "    \n",
      "    def __len__(self):\n",
      "        return len(self.imgs)\n",
      "\n",
      "\n",
      "def get_transform(train):\n",
      "    transforms = []\n",
      "    transforms.append(T.ToTensor())\n",
      "    if train:\n",
      "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
      "    return T.Compose(transforms)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description='PyTorch Object Detection Training')\n",
      "    parser.add_argument('--data_path', \n",
      "                        default='./BuildData/', help='the path to the dataset')\n",
      "    parser.add_argument('--batch_size', \n",
      "                        default=2, type=int)\n",
      "    parser.add_argument('--epochs', \n",
      "                        default=10, type=int, help='number of total epochs to run')\n",
      "    parser.add_argument('--workers', \n",
      "                        default=4, type=int, help='number of data loading workers')\n",
      "    parser.add_argument('--learning_rate', \n",
      "                        default=0.005, type=float, help='initial learning rate')\n",
      "    parser.add_argument('--momentum', \n",
      "                        default=0.9, type=float, help='momentum')\n",
      "    parser.add_argument('--weight_decay', \n",
      "                        default=0.0005, type=float, help='weight decay (default: 1e-4)')\n",
      "    parser.add_argument('--lr_step_size', \n",
      "                        default=3, type=int, help='decrease lr every step-size epochs')\n",
      "    parser.add_argument('--lr_gamma', \n",
      "                        default=0.1, type=float, help='decrease lr by a factor of lr-gamma')\n",
      "    parser.add_argument('--print_freq', \n",
      "                        default=10, type=int, help='print frequency')\n",
      "    parser.add_argument('--output_dir', \n",
      "                        default='outputs', help='path where to save')\n",
      "    parser.add_argument('--anchor_sizes', \n",
      "                        default='16', type=str, help='anchor sizes')\n",
      "    parser.add_argument('--anchor_aspect_ratios', \n",
      "                        default= '1.0', type=str, help='anchor aspect ratios')\n",
      "    parser.add_argument('--rpn_nms_thresh', \n",
      "                        default= 0.7, type=float,  help='NMS threshold used for postprocessing the RPN proposals')\n",
      "    parser.add_argument('--box_nms_thresh', \n",
      "                        default= 0.5, type=float,  help='NMS threshold for the prediction head. Used during inference')\n",
      "    parser.add_argument('--box_score_thresh', \n",
      "                        default= 0.05, type=float,  help='during inference only return proposals' \n",
      "                        'with a classification score greater than box_score_thresh')\n",
      "    parser.add_argument('--box_detections_per_img', \n",
      "                        default= 100, type=int,  help='maximum number of detections per image, for all classes')\n",
      "    args = parser.parse_args() \n",
      "\n",
      "data_path = args.data_path\n",
      "\n",
      "# use our dataset and defined transformations\n",
      "dataset = BuildDataset(data_path, get_transform(train=True))\n",
      "dataset_test = BuildDataset(data_path, get_transform(train=False))\n",
      "\n",
      "# split the dataset in train and test set\n",
      "indices = torch.randperm(len(dataset)).tolist()\n",
      "dataset = torch.utils.data.Subset(dataset, indices[:-100])\n",
      "dataset_test = torch.utils.data.Subset(dataset_test, indices[-100:])\n",
      "\n",
      "batch_size = args.batch_size\n",
      "workers = args.workers\n",
      "\n",
      "# define training and validation data loaders\n",
      "data_loader = torch.utils.data.DataLoader(\n",
      "    dataset, batch_size=2, shuffle=True, num_workers=workers,\n",
      "    collate_fn=utils.collate_fn)\n",
      "\n",
      "data_loader_test = torch.utils.data.DataLoader(\n",
      "    dataset_test, batch_size=2, shuffle=False, num_workers=workers,\n",
      "    collate_fn=utils.collate_fn)\n",
      "\n",
      "# our dataset has two classes only - background and out of stock\n",
      "num_classes = 2\n",
      "\n",
      "rpn_nms_threshold = args.rpn_nms_thresh\n",
      "box_nms_threshold = args.box_nms_thresh\n",
      "box_score_threshold = args.box_score_thresh\n",
      "num_box_detections = args.box_detections_per_img\n",
      "\n",
      "# load pre-trained maskRCNN model\n",
      "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, \n",
      "                                                           rpn_nms_thresh=rpn_nms_threshold,\n",
      "                                                           box_nms_thresh=box_nms_threshold, \n",
      "                                                           box_score_thresh=box_score_threshold,\n",
      "                                                           box_detections_per_img=num_box_detections)\n",
      "\n",
      "# get number of input features for the classifier\n",
      "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
      "\n",
      "# replace the pre-trained head with a new one\n",
      "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
      "\n",
      "anchor_sizes = args.anchor_sizes\n",
      "anchor_sizes = tuple([float(i) for i in anchor_sizes.split(',')])\n",
      "anchor_aspect_ratios = args.anchor_aspect_ratios\n",
      "anchor_aspect_ratios = tuple([float(i) for i in anchor_aspect_ratios.split(',')])\n",
      "\n",
      "# create an anchor_generator for the FPN which by default has 5 outputs\n",
      "anchor_generator = AnchorGenerator(\n",
      "    sizes=tuple([anchor_sizes for _ in range(5)]),\n",
      "    aspect_ratios=tuple([anchor_aspect_ratios for _ in range(5)]))\n",
      "model.rpn.anchor_generator = anchor_generator\n",
      "\n",
      "# get number of input features for the RPN returned by FPN (256)\n",
      "in_channels = model.backbone.out_channels\n",
      "\n",
      "# replace the RPN head \n",
      "model.rpn.head = RPNHead(in_channels, anchor_generator.num_anchors_per_location()[0])\n",
      "\n",
      "# turn off masks since dataset only has bounding boxes\n",
      "model.roi_heads.mask_roi_pool = None\n",
      "\n",
      "# train on the GPU or on the CPU, if a GPU is not available\n",
      "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
      "\n",
      "# move model to the right device\n",
      "model.to(device)\n",
      "\n",
      "learning_rate = args.learning_rate\n",
      "momentum = args.momentum\n",
      "weight_decay = args.weight_decay\n",
      "\n",
      "# construct an optimizer\n",
      "params = [p for p in model.parameters() if p.requires_grad]\n",
      "optimizer = torch.optim.SGD(params, lr=learning_rate,\n",
      "                            momentum=momentum, weight_decay=weight_decay)\n",
      "\n",
      "lr_step_size = args.lr_step_size\n",
      "lr_gamma = args.lr_gamma\n",
      "\n",
      "# and a learning rate scheduler\n",
      "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
      "                                               step_size=lr_step_size,\n",
      "                                               gamma=lr_gamma)\n",
      "\n",
      "# number of training epochs\n",
      "num_epochs = args.epochs\n",
      "print_freq = args.print_freq\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    # train for one epoch, printing every 10 iterations\n",
      "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=print_freq)\n",
      "    # update the learning rate\n",
      "    lr_scheduler.step()\n",
      "    # evaluate on the test dataset after every epoch\n",
      "    evaluate(model, data_loader_test, device=device)\n",
      "    \n",
      "#save model\n",
      "torch.save(model.state_dict(), os.path.join(args.output_dir, 'model_latest.pth'))\n",
      "\n",
      "print(\"That's it!\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./train.py', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A Pytorch Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the Pytorch class and also a widget to visualize a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'use_docker' parameter will be deprecated. Please use 'environment_definition' instead.\n",
      "WARNING:azureml.train.estimator:framework_version is not specified, defaulting to version 1.1.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "script_params = {\n",
    "    '--data_path': '/datadrive/torchvisionOD/BuildData/',\n",
    "    '--workers': 8,\n",
    "    '--learning_rate' : 0.005,\n",
    "    '--epochs' : 5,\n",
    "    '--anchor_sizes' : '16,32,64,128,256,512',\n",
    "    '--anchor_aspect_ratios' : '0.25,0.5,1.0,2.0',\n",
    "    '--rpn_nms_thresh' : 0.5,\n",
    "    '--box_nms_thresh' : 0.3,\n",
    "    '--box_score_thresh' : 0.10    \n",
    "}\n",
    "\n",
    "estimator = PyTorch(source_directory='.', \n",
    "                    script_params=script_params,\n",
    "                    compute_target='local',\n",
    "                    entry_script='train.py',\n",
    "                    use_docker=False,\n",
    "                    user_managed=True,\n",
    "                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.run_config.environment.python.interpreter_path = '/data/anaconda/envs/torchdetectaml/bin/python'\n",
    "estimator.run_config.history.snapshot_project = False\n",
    "# estimator.run_config.environment.environment_variables[\"PYTHONPATH\"] = \"$PYTHONPATH:/datadrive/cocoapi/PythonAPI/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest estimator is to submit the current folder to the local computer. Estimator by default will attempt to use Docker-based execution. Let's turn that off for now. It then builds a conda environment locally, installs Azure ML SDK in it, and runs your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e7f0fe773a416eb66a3a38f33f9fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = exp.submit(estimator)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: torchvision_1560532657_a125710f\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/fboyluamlrg/providers/Microsoft.MachineLearningServices/workspaces/fboyluamlws/experiments/torchvision/runs/torchvision_1560532657_a125710f\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Epoch: [0]  [  0/424]  eta: 0:20:10  lr: 0.000017  loss: 2.1649 (2.1649)  loss_classifier: 1.3078 (1.3078)  loss_box_reg: 0.0034 (0.0034)  loss_objectness: 0.6881 (0.6881)  loss_rpn_box_reg: 0.1655 (0.1655)  time: 2.8542  data: 0.2664  max mem: 3722\n",
      "Epoch: [0]  [ 10/424]  eta: 0:13:57  lr: 0.000135  loss: 1.9251 (1.8368)  loss_classifier: 1.0036 (0.9059)  loss_box_reg: 0.0087 (0.0082)  loss_objectness: 0.6887 (0.6900)  loss_rpn_box_reg: 0.2646 (0.2328)  time: 2.0226  data: 0.0319  max mem: 5977\n",
      "Epoch: [0]  [ 20/424]  eta: 0:13:30  lr: 0.000253  loss: 1.2237 (1.4852)  loss_classifier: 0.1816 (0.5322)  loss_box_reg: 0.0095 (0.0109)  loss_objectness: 0.6887 (0.6893)  loss_rpn_box_reg: 0.2646 (0.2527)  time: 1.9634  data: 0.0069  max mem: 6342\n",
      "Epoch: [0]  [ 30/424]  eta: 0:13:21  lr: 0.000371  loss: 1.0649 (1.3427)  loss_classifier: 0.1061 (0.4030)  loss_box_reg: 0.0152 (0.0151)  loss_objectness: 0.6840 (0.6870)  loss_rpn_box_reg: 0.2467 (0.2377)  time: 2.0388  data: 0.0065  max mem: 6503\n",
      "Epoch: [0]  [ 40/424]  eta: 0:12:58  lr: 0.000489  loss: 1.0467 (1.2839)  loss_classifier: 0.1368 (0.3478)  loss_box_reg: 0.0284 (0.0248)  loss_objectness: 0.6769 (0.6834)  loss_rpn_box_reg: 0.1810 (0.2279)  time: 2.0475  data: 0.0067  max mem: 6503\n",
      "Epoch: [0]  [ 50/424]  eta: 0:12:29  lr: 0.000607  loss: 1.1195 (1.2565)  loss_classifier: 0.1606 (0.3146)  loss_box_reg: 0.0782 (0.0380)  loss_objectness: 0.6695 (0.6794)  loss_rpn_box_reg: 0.1975 (0.2245)  time: 1.9596  data: 0.0057  max mem: 6503\n",
      "Epoch: [0]  [ 60/424]  eta: 0:12:06  lr: 0.000725  loss: 1.0413 (1.2154)  loss_classifier: 0.1152 (0.2800)  loss_box_reg: 0.0825 (0.0447)  loss_objectness: 0.6605 (0.6758)  loss_rpn_box_reg: 0.1946 (0.2148)  time: 1.9311  data: 0.0058  max mem: 6503\n",
      "Epoch: [0]  [ 70/424]  eta: 0:11:46  lr: 0.000843  loss: 0.9681 (1.1818)  loss_classifier: 0.0963 (0.2553)  loss_box_reg: 0.0622 (0.0483)  loss_objectness: 0.6460 (0.6697)  loss_rpn_box_reg: 0.1667 (0.2086)  time: 1.9753  data: 0.0059  max mem: 6503\n",
      "Epoch: [0]  [ 80/424]  eta: 0:11:25  lr: 0.000961  loss: 0.9855 (1.1621)  loss_classifier: 0.1089 (0.2386)  loss_box_reg: 0.0744 (0.0564)  loss_objectness: 0.6092 (0.6602)  loss_rpn_box_reg: 0.1816 (0.2070)  time: 1.9843  data: 0.0057  max mem: 6503\n",
      "Epoch: [0]  [ 90/424]  eta: 0:11:03  lr: 0.001080  loss: 1.0100 (1.1456)  loss_classifier: 0.1192 (0.2274)  loss_box_reg: 0.0998 (0.0651)  loss_objectness: 0.5678 (0.6498)  loss_rpn_box_reg: 0.1804 (0.2033)  time: 1.9527  data: 0.0058  max mem: 6503\n",
      "Epoch: [0]  [100/424]  eta: 0:10:44  lr: 0.001198  loss: 0.9326 (1.1201)  loss_classifier: 0.1156 (0.2152)  loss_box_reg: 0.1183 (0.0701)  loss_objectness: 0.5175 (0.6345)  loss_rpn_box_reg: 0.1663 (0.2004)  time: 1.9738  data: 0.0057  max mem: 6503\n",
      "Epoch: [0]  [110/424]  eta: 0:10:27  lr: 0.001316  loss: 0.9386 (1.1060)  loss_classifier: 0.1156 (0.2072)  loss_box_reg: 0.1389 (0.0793)  loss_objectness: 0.4588 (0.6170)  loss_rpn_box_reg: 0.2092 (0.2025)  time: 2.0509  data: 0.0057  max mem: 6503\n",
      "Epoch: [0]  [120/424]  eta: 0:10:07  lr: 0.001434  loss: 0.8614 (1.0829)  loss_classifier: 0.0956 (0.1970)  loss_box_reg: 0.1040 (0.0787)  loss_objectness: 0.4443 (0.6043)  loss_rpn_box_reg: 0.2092 (0.2029)  time: 2.0522  data: 0.0058  max mem: 6503\n",
      "Epoch: [0]  [130/424]  eta: 0:09:47  lr: 0.001552  loss: 0.8577 (1.0691)  loss_classifier: 0.0919 (0.1899)  loss_box_reg: 0.0892 (0.0802)  loss_objectness: 0.4558 (0.5928)  loss_rpn_box_reg: 0.2185 (0.2062)  time: 2.0085  data: 0.0056  max mem: 6503\n"
     ]
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/60_control_log.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'logs/azureml/azureml.log',\n",
       " 'outputs/model_latest.pth']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "maxluk"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:torchdetectaml]",
   "language": "python",
   "name": "conda-env-torchdetectaml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "msauthor": "minxia"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
